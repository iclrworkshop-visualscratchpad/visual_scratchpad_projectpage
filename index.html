<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="VisualScratchpad: Grounding Visual Concepts in Large Vision Language Models">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="sparse autoencoders, visual concepts, multimodal large language models>
  <!-- TODO: List all authors -->
  <meta name=" author" content="Anonimous">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="VisualScratchpad: Grounding Visual Concepts in Large Vision Language Models">
  <meta name="citation_author" content="Anonimous">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="ArXiv">
  <meta name="citation_pdf_url" content="">

  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>VisualScratchpad: Grounding Visual Concepts in Large Vision Language Models | Academic Research</title>

  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>&#x1F52C;</text></svg>">
  <!-- <link rel="apple-touch-icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>&#x1F52C;</text></svg>"> -->



  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">

  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300;400;500;600;700&display=swap"
    rel="stylesheet">

  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "VisualScratchpad: Grounding Visual Concepts in Large Vision Language Models",
    "description": "To make model internals more accessible and enable systematic debugging, we introduce VisualScratchpad, an interactive interface for visual concept analysis during inference.",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>

  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>

<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">
    <section class="hero">
      <div class="hero-body" style="padding: 4rem 1.5rem;">
        <div class="container is-max-widescreen">
          <div class="columns is-centered">
            <div class="column has-text-centered" style="max-width: 1200px;">
              <!-- Modern Title -->
              <h1 class="title is-1 publication-title">
                VisualScratchpad: Grounding Visual Concepts in Large Vision Language Models
              </h1>

              <!-- Venue -->
              <div style="margin-bottom: 2rem; display: flex; align-items: center; justify-content: center; gap: 1rem;">
                <div
                  style="font-family: 'Space Grotesk', sans-serif; font-weight: 600; font-size: 1.3rem; color: #333;">
                  Submission to ICLR 2026 Workshop on <br> Principled Design for Trustworthy AI - Interpretability,
                  Robustness,
                  and Safety across Modalities
                </div>
              </div>

              <!-- Authors -->
              <div class="is-size-5 publication-authors"
                style="font-family: 'Space Grotesk', sans-serif; font-weight: 400; font-size: 1.2rem; margin-bottom: 1.5rem;">
                <span class="author-block">
                  <a href="" target="_blank">Anonimous</a>

                </span>
              </div>

              <!-- Affiliations -->
              <!-- <div class="is-size-6 publication-authors"
                style="font-family: 'Space Grotesk', sans-serif; font-weight: 300; font-size: 1rem; margin-bottom: 2rem; line-height: 1.6; color: #666;">
                <span class="author-block">
                  <sup>1</sup>Anonimous, <sup>2</sup>Anonimous, <span style="font-style: italic;">*Correspondence</span>
                </span>
              </div> -->


              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- TODO: Add your supplementary material PDF or remove this section -->
                  <!-- <span class="link-block">
                    <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/iclrworkshop-visualscratchpad/visual_scratchpad" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- Interactive Demo -->
                  <span class="link-block">
                    <a href="#interactive-demo" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-play-circle"></i>
                      </span>
                      <span>Demo</span>
                    </a>
                  </span>

                  <!-- TODO: Update with your arXiv paper ID -->
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Teaser image and description -->
    <section class="hero teaser" style="background-color: white;">
      <div class="container is-max-widescreen">
        <div class="hero-body">
          <div class="columns is-centered">
            <div class="column" style="max-width: 80%;">
              <!-- TLDR Section -->
              <img src="static/images/method.png"
                alt="VisualScratchpad: Grounding Visual Concepts in Large Vision Language Models"
                style="width: 100%; max-width: 1200px; height: auto;" loading="lazy" />
              <!-- Caption style description -->
              <p class="teaser-caption"
                style="text-align: left; padding: 1rem 0; margin: 1rem 0; font-size: 1.1rem; line-height: 1.5; color: #666;">
                To make vision-language model internals more accessible and enable systematic debugging, we introduce
                <strong>VisualScratchpad</strong>, an interactive interface for visual concept analysis during
                inference.
                <strong>A.</strong> During inference in a vision-language model, we extract
                the intermediate representation <strong>z</strong> from the vision encoder. <strong>B.</strong> A sparse
                autoencoder processes <strong>z</strong> to
                produce concept activations. The attention map from output text tokens to image tokens is applied
                at the patch level to weight these activations. Latents exhibiting similar activation patterns across
                output tokens are then clustered and visualized in a token-latent heatmap. <strong>C.</strong>
                The causal influence
                of these concepts on the model's output can be evaluated through latent ablation.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End teaser image and description -->


    <!-- Interactive Demo -->
    <section id="interactive-demo" , class="hero is-small is-light">
      <div class=" hero-body">
        <div class="container">
          <h2 class="title is-3">Interactive Demo</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="publication-video">
                <!-- TODO: Replace with your YouTube video ID -->
                <iframe src="https://drive.google.com/file/d/1JfLM15BaZ93McqeG46B03Fj6cX5evVc4/preview" frameborder="0"
                  allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End youtube video -->



    <!-- Image carousel -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <!-- <h2 class="title is-3">Instructions for Interactive Demo</h2> -->
          <div class="carousel-indicator" style="text-align: center; margin-bottom: 1rem;">
            <span id="carousel-counter" style="font-size: 1.1rem; font-weight: 600;"></span>
          </div>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- TODO: Replace with your research result images -->
              <img src="static/images/vspd_1.png" alt="First research result visualization" loading="lazy" />
              <!-- TODO: Replace with description of this result -->
              <h2 class="subtitle" style="text-align: left; color: #333;">
                The user first explores SAE statistics such as sparsity, activation levels, and label entropy to assess
                training quality. A well-trained SAE typically exhibits clusters of latents that are highly sparse and
                strongly activating.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/vspd_2.png" alt="Second research result visualization" loading="lazy" />
              <h2 class="subtitle" style="text-align: left; color: #333;">
                The user then identifies meaningful latents from the clustering results of the decoder weights.
                Meaningful latents tend to form clear, coherent clusters.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/vspd_3.png" alt="Third research result visualization" loading="lazy" />
              <h2 class="subtitle" style="text-align: left; color: #333;">
                The user provides an input image to the VLM and specifies an input prompt. The interface then displays
                the tokenized outputs, along with the tokenized system prompt and input prompt.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/vspd_4.png" alt="Fourth research result visualization" loading="lazy" />
              <h2 class="subtitle" style="text-align: left; color: #333;">
                The user can steer the model by selecting specific latents and assigning new values, then running the
                VLM again with the steered latents applied.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/vspd_5.png" alt="Fifth research result visualization" loading="lazy" />
              <h2 class="subtitle" style="text-align: left; color: #333;">
                In the right panel, the user can view the VLM's output attention, showing the spatial regions of the
                input image that the model attends to for each output token or question token.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/vspd_6.png" alt="Sixth research result visualization" loading="lazy" />
              <h2 class="subtitle" style="text-align: left; color: #333;">
                VisualScratchpad provides a token-latent heatmap view, where latents are clustered and visualized
                according to their activation similarity across output tokens.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/vspd_7.png" alt="Seventh research result visualization" loading="lazy" />
              <h2 class="subtitle" style="text-align: left; color: #333;">
                Users may select individual clusters to steer the model based on the latents grouped within those
                clusters.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/vspd_8.png" alt="Eighth research result visualization" loading="lazy" />
              <h2 class="subtitle" style="text-align: left; color: #333;">
                This panel visualizes each latent's activation weighted by the attention map for the selected token,
                allowing users to identify which latents contribute most strongly to the production of that token.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/vspd_9.png" alt="Eighth research result visualization" loading="lazy" />
              <h2 class="subtitle" style="text-align: left; color: #333;">
                Users can optionally apply attention weighting to the SAE activation values and filter out noisy latents
                using sparsity and mean activation thresholds.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/vspd_10.png" alt="Eighth research result visualization" loading="lazy" />
              <h2 class="subtitle" style="text-align: left; color: #333;">
                In the right panel, users can inspect how the selected latent activates on the input image and on
                reference images through their spatial attributions.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End image carousel -->



    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3 has-text-centered">Result Highlights</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column is-two-thirds">
              <img src="static/images/results.png" alt="VisualScratchpad Result Highlights"
                style="width: 100%; max-width: 1000px; height: auto;" loading="lazy" />
              <div class="content has-text-justified" style="margin-top: 2rem;">
                <p>
                  <strong>We present three failure cases in which LLaVA-Next-8B initially produces incorrect
                    outputs.</strong>
                  <br>Case 1: Even when the vision encoder successfully captures the correct cue, the model may fail to
                  utilize it; providing a more explicit description in the input prompt corrects the output.
                  <br>Case 2: When the model relies on a misleading visual cue, removing that cue leads to a change in
                  prediction.
                  <br>Case 3: Although the vision encoder may capture multiple plausible visual concepts, the model
                  often relies disproportionately on the most dominant one.
              </div>
            </div>
          </div>

        </div>
      </div>
    </section>



    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3 has-text-centered">How does VisualScratchpad work?</h2>
          <div class="columns is-centered has--centered">
            <div class="column is-two-thirds">
              <img src="static/images/method2.png" alt="VisualScratchpad Method Overview"
                style="width: 100%; max-width: 1000px; height: auto;" loading="lazy" />
              <div class="content has-text-justified" style="margin-top: 2rem;">
                <strong>Attention-based concept re-ranking</strong>
                <p>
                  <strong>A.</strong> SAEs return latent activations
                  for each image patch. <strong>B.</strong>
                  Image-level activations
                  can be computed by naïvely averaging activations across all patches, or <strong>C.</strong> by
                  applying a
                  weighted average where the text-to-image attention map serves as the weighting coefficient, promoting
                  concepts relevant to the text tokens to the top of the ranking. The bottom row shows the top-ranked
                  concept obtained from the corresponding method
                </p>
              </div>
            </div>
          </div>




          <div class="columns is-centered has--centered">
            <div class="column is-two-thirds">
              <img src="static/images/token-latent.png" alt="VisualScratchpad Method Overview"
                style="width: 100%; max-width: 1000px; height: auto;" loading="lazy" />
              <div class="content has-text-justified" style="margin-top: 2rem;">
                <strong>Token-Latent heatmap visualization and clustering </strong>
                <p>
                  Raw values are difficult to analyze, so we normalize in column-wise to show if the latent is
                  specifically attended by certain tokens or by overall tokens. Moreover, we cluster and sort by
                  activation correlation in column-wise, using hierarchical clustering.
                </p>
              </div>
            </div>
          </div>


          <div class="columns is-centered has--centered">
            <div class="column is-two-thirds">
              <img src="static/images/concept_clusters.png" alt="VisualScratchpad Method Overview"
                style="width: 100%; max-width: 1000px; height: auto;" loading="lazy" />
              <img src="static/images/causal_inference.png" alt="VisualScratchpad Method Overview"
                style="width: 100%; max-width: 1000px; height: auto;" loading="lazy" />
              <div class="content has-text-justified" style="margin-top: 2rem;">
                <strong>Causal inference</strong>
                <p>
                  Ablating latent clusters corresponding to distinct semantic topics removes those topics from the
                  generated output, demonstrating their causal role in shaping the model's
                  predictions.
                </p>
              </div>
            </div>
          </div>



        </div>




      </div>
    </section>






    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">

              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank">Academic Project Page Template</a> which was adopted from the <a
                  href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                You are free to borrow the source code of this website, we just ask that you link back to this page in
                the footer. <br> This website is licensed under a <a rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                  Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>

            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>

</html>